# -*- coding: utf-8 -*-
"""mobilenet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/155NmKmaag0sK3m7JBVXzJhG2KSTDjEwN
"""

# -*- coding: utf-8 -*-
"""MobileNet_Model.ipynb"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d tourist55/alzheimers-dataset-4-class-of-images

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow-addons
!pip install tensorflow

import tensorflow as tf
from zipfile import ZipFile
import os, glob
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

print("TensorFlow Version:", tf.__version__)

file_name = "/content/alzheimers-dataset-4-class-of-images.zip"
with ZipFile(file_name, 'r') as zip:
    zip.extractall("/content/")
    print('Done')
    extracted_dir = "/content/Alzheimer_s Dataset"
print("Contents of extracted directory:")
print(os.listdir(extracted_dir))

AUTOTUNE = tf.data.experimental.AUTOTUNE
base_dir = "/content/Alzheimer_s Dataset"
train_dir = os.path.join(extracted_dir, "train")
test_dir = os.path.join(extracted_dir, "test")

CLASSES = [ 'NonDemented',
            'VeryMildDemented',
            'MildDemented',
            'ModerateDemented']
IMG_SIZE = 224
IMAGE_SIZE = [224, 224]
DIM = (IMG_SIZE, IMG_SIZE)

BATCH_SIZE = 32

# Set a random seed for reproducibility
seed_value = 1337

# Create separate data generators for training and validation with augmentation
train_datagen = IDG(
    rescale=1./255,
    zoom_range=[.99, 1.01],
    horizontal_flip=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    fill_mode='constant',
    validation_split=0.2,
    data_format='channels_last'
)

train_ds = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',  # Assuming you're doing multi-class classification
    subset='training',
    seed=seed_value
)

val_ds = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',  # Assuming you're doing multi-class classification
    subset='validation',
    seed=seed_value
)

# Count images per category
datasets = ["train", "test"]
counts = {cls: 0 for cls in CLASSES}
for dataset in datasets:
    for cls in CLASSES:
        counts[cls] += len(glob.glob(os.path.join(extracted_dir, dataset, cls, '*.jpg')))

data = {'Category': list(counts.keys()), 'Count': list(counts.values())}
sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))
ax = sns.barplot(x='Category', y='Count', data=data, palette='viridis')
ax.set_title('Number of Images per Category')
plt.show()

# Class weights
total_samples = sum(counts.values())
class_weights = {i: total_samples / (4 * count) for i, count in enumerate(counts.values())}

# New section: MobileNet model with the same data augmentation
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization

# Load the MobileNet model
base_model = MobileNet(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights="imagenet")

# Freeze the base model
for layer in base_model.layers:
    layer.trainable = False
for layer in base_model.layers[-15:]:
    layer.trainable = True

# Create a new model on top
model_mobilenet = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    BatchNormalization(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    BatchNormalization(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    BatchNormalization(),
    Dense(len(CLASSES), activation='softmax')
])

# Compile the model
model_mobilenet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                        loss='categorical_crossentropy',
                        metrics=['accuracy'])

# Print the model summary
model_mobilenet.summary()

# Set up callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history_mobilenet = model_mobilenet.fit(
    train_ds,
    steps_per_epoch=train_ds.samples // BATCH_SIZE,
    epochs=30,
    validation_data=val_ds,
    validation_steps=val_ds.samples // BATCH_SIZE,
    class_weight=class_weights,  # Utilize the calculated class weights
    callbacks=[reduce_lr, early_stopping]
)

# Evaluate the model on the test set
test_ds = train_datagen.flow_from_directory(
    test_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    seed=seed_value
)

test_results_mobilenet = model_mobilenet.evaluate(test_ds)
print("Test Loss:", test_results_mobilenet[0])
print("Test Accuracy:", test_results_mobilenet[1])

# Plot training history
fig, ax = plt.subplots(1, 2, figsize=(20, 5))
ax = ax.ravel()

for i, met in enumerate(['accuracy', 'loss']):
    ax[i].plot(history_mobilenet.history[met])
    ax[i].plot(history_mobilenet.history['val_' + met])
    ax[i].set_title(f'MobileNet Model {met.capitalize()}')
    ax[i].set_xlabel('Epochs')
    ax[i].set_ylabel(met.capitalize())
    ax[i].legend(['Train', 'Validation'])

plt.show()

# Mount Google Drive

from google.colab import drive
drive.mount('/content/drive')

# Specify the path in your Google Drive where you want to save the model
import os
model_save_path = '/content/drive/My Drive/mobilenet.h5'

# Save the model
model_mobilenet.save(model_save_path)
print(f"Model saved to {model_save_path}")