# -*- coding: utf-8 -*-
"""finalscratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YbfGFuD84Hk7xR3r7cPM6LAsdBY3c93W
"""

# -*- coding: utf-8 -*-
"""Sequential_Model_Scratch.ipynb"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d tourist55/alzheimers-dataset-4-class-of-images

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow-addons
!pip install tensorflow

import tensorflow as tf
from zipfile import ZipFile
import os, glob
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG
from tensorflow.keras.callbacks import ReduceLROnPlateau

print("TensorFlow Version:", tf.__version__)

file_name = "/content/alzheimers-dataset-4-class-of-images.zip"
with ZipFile(file_name, 'r') as zip:
    zip.extractall("/content/")
    print('Done')
    extracted_dir = "/content/Alzheimer_s Dataset"
print("Contents of extracted directory:")
print(os.listdir(extracted_dir))

AUTOTUNE = tf.data.experimental.AUTOTUNE
base_dir = "/content/Alzheimer_s Dataset"
train_dir = os.path.join(extracted_dir, "train")
test_dir = os.path.join(extracted_dir, "test")

CLASSES = [ 'NonDemented',
            'VeryMildDemented',
            'MildDemented',
            'ModerateDemented']
IMG_SIZE = 299
IMAGE_SIZE = [299, 299]
DIM = (IMG_SIZE, IMG_SIZE)

BATCH_SIZE = 32

# Set a random seed for reproducibility
seed_value = 1337

# Create separate data generators for training and validation with augmentation
train_datagen = IDG(
    rescale=1./255,
    zoom_range=[.99, 1.01],
    horizontal_flip=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    fill_mode='constant',
    validation_split=0.2,
    data_format='channels_last'
)

train_ds = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',  # Assuming you're doing multi-class classification
    subset='training',
    seed=seed_value
)

val_ds = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',  # Assuming you're doing multi-class classification
    subset='validation',
    seed=seed_value
)

# Count images per category
datasets = ["train", "test"]
counts = {cls: 0 for cls in CLASSES}
for dataset in datasets:
    for cls in CLASSES:
        counts[cls] += len(glob.glob(os.path.join(extracted_dir, dataset, cls, '*.jpg')))

data = {'Category': list(counts.keys()), 'Count': list(counts.values())}
sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))
ax = sns.barplot(x='Category', y='Count', data=data, palette='viridis')
ax.set_title('Number of Images per Category')
plt.show()

# Class weights
total_samples = sum(counts.values())
class_weights = {i: total_samples / (4 * count) for i, count in enumerate(counts.values())}

# New section: Sequential model from scratch with 12 layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

# Initialize a Sequential model
model_scratch = Sequential()

# Add layers
model_scratch.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)))
model_scratch.add(MaxPooling2D(pool_size=(2, 2)))

model_scratch.add(Conv2D(64, (3, 3), activation='relu'))
model_scratch.add(MaxPooling2D(pool_size=(2, 2)))

model_scratch.add(Conv2D(128, (3, 3), activation='relu'))
model_scratch.add(MaxPooling2D(pool_size=(2, 2)))

model_scratch.add(Conv2D(256, (3, 3), activation='relu'))
model_scratch.add(MaxPooling2D(pool_size=(2, 2)))


model_scratch.add(Flatten())

model_scratch.add(Dense(512, activation='relu'))
model_scratch.add(Dropout(0.5))

model_scratch.add(Dense(256, activation='relu'))
model_scratch.add(Dropout(0.5))

model_scratch.add(Dense(128, activation='relu'))
model_scratch.add(Dropout(0.5))

model_scratch.add(Dense(64, activation='relu'))
model_scratch.add(Dropout(0.5))
model_scratch.add(BatchNormalization())

model_scratch.add(Dense(len(CLASSES), activation='softmax'))

# Compile the model
model_scratch.compile(optimizer='adam',
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

# Print the model summary
model_scratch.summary()

# Set up callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)

# Train the model
history_scratch = model_scratch.fit(
    train_ds,
    steps_per_epoch=train_ds.samples // BATCH_SIZE,
    epochs=20,
    validation_data=val_ds,
    validation_steps=val_ds.samples // BATCH_SIZE,
    class_weight=class_weights,  # Utilize the calculated class weights
    callbacks=[reduce_lr]
)

# Evaluate the model on the test set
test_ds = train_datagen.flow_from_directory(
    test_dir,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    seed=seed_value
)

test_results_scratch = model_scratch.evaluate(test_ds)
print("Test Loss:", test_results_scratch[0])
print("Test Accuracy:", test_results_scratch[1])

# Plot training history
fig, ax = plt.subplots(1, 2, figsize=(20, 5))
ax = ax.ravel()

for i, met in enumerate(['accuracy', 'loss']):
    ax[i].plot(history_scratch.history[met])
    ax[i].plot(history_scratch.history['val_' + met])
    ax[i].set_title(f'Sequential Model {met.capitalize()}')
    ax[i].set_xlabel('Epochs')
    ax[i].set_ylabel(met.capitalize())
    ax[i].legend(['Train', 'Validation'])

plt.show()

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Specify the path in your Google Drive where you want to save the model
import os
model_save_path = '/content/drive/My Drive/scratchmodel.h5'

# Save the model
model_scratch.save(model_save_path)
print(f"Model saved to {model_save_path}")